name: kafka
description: Apache Kafka event streaming platform for building real-time data pipelines and streaming applications
category: messaging
version: "latest"

# Default configuration
defaults:
  image: confluentinc/cp-kafka:latest
  zookeeper_image: confluentinc/cp-zookeeper:latest
  port: 9092
  zookeeper_port: 2181
  ui_port: 8080
  memory_limit: 1024m
  zookeeper_memory_limit: 256m
  auto_create_topics: true
  num_partitions: 1
  replication_factor: 1
  default_topics_enabled: true

# Environment variables this service provides
environment:
  KAFKA_HOST: localhost
  KAFKA_PORT: "${KAFKA_PORT:-9092}"
  KAFKA_BOOTSTRAP_SERVERS: "localhost:${KAFKA_PORT:-9092}"
  KAFKA_UI_PORT: "${KAFKA_UI_PORT:-8080}"
  ZOOKEEPER_HOST: localhost
  ZOOKEEPER_PORT: "${ZOOKEEPER_PORT:-2181}"
  ZOOKEEPER_CONNECT: "localhost:${ZOOKEEPER_PORT:-2181}"

# Spring Boot configuration
spring_config:
  properties:
    - "spring.kafka.bootstrap-servers=localhost:${KAFKA_PORT:-9092}"
    - "spring.kafka.consumer.group-id=${spring.application.name:local-app}"
    - "spring.kafka.consumer.auto-offset-reset=earliest"
    - "spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer"
    - "spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer"
    - "spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer"
    - "spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer"
  yaml: |
    spring:
      kafka:
        bootstrap-servers: localhost:${KAFKA_PORT:-9092}
        consumer:
          group-id: ${spring.application.name:local-app}
          auto-offset-reset: earliest
          key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
          value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
          properties:
            spring.json.trusted.packages: "*"
        producer:
          key-serializer: org.apache.kafka.common.serialization.StringSerializer
          value-serializer: org.apache.kafka.common.serialization.StringSerializer
        admin:
          properties:
            bootstrap.servers: localhost:${KAFKA_PORT:-9092}

# Docker-specific configuration
docker:
  services:
    zookeeper:
      image: confluentinc/cp-zookeeper:latest
      restart: unless-stopped
      networks:
        - dev-stack
      memory_limit: 256m
      environment:
        - ZOOKEEPER_CLIENT_PORT=2181
        - ZOOKEEPER_TICK_TIME=2000
        - ZOOKEEPER_SYNC_LIMIT=2
      health_check:
        test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
        interval: 10s
        timeout: 5s
        retries: 5
        start_period: 30s
    
    kafka:
      image: confluentinc/cp-kafka:latest
      depends_on:
        - zookeeper
      restart: unless-stopped
      networks:
        - dev-stack
      memory_limit: 1024m
      environment:
        - KAFKA_BROKER_ID=1
        - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
        - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:${KAFKA_PORT:-9092}
        - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
        - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
        - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
        - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
        - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
        - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
        - KAFKA_NUM_PARTITIONS=3
        - KAFKA_DEFAULT_REPLICATION_FACTOR=1
        - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
        - KAFKA_LOG_RETENTION_HOURS=168
        - KAFKA_LOG_RETENTION_BYTES=1073741824
        - KAFKA_LOG_SEGMENT_BYTES=1073741824
        - KAFKA_LOG_CLEANUP_POLICY=delete
        - KAFKA_HEAP_OPTS=-Xmx512M -Xms256M
      health_check:
        test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 60s
    
    kafka-ui:
      image: provectuslabs/kafka-ui:latest
      depends_on:
        - kafka
      restart: unless-stopped
      networks:
        - dev-stack
      memory_limit: 256m
      environment:
        - KAFKA_CLUSTERS_0_NAME=dev-stack
        - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
        - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
        - DYNAMIC_CONFIG_ENABLED=true
      health_check:
        test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
        interval: 30s
        timeout: 10s
        retries: 3
        start_period: 30s
    
    kafka-init:
      image: confluentinc/cp-kafka:latest
      depends_on:
        - kafka
      restart: "no"
      networks:
        - dev-stack
      command: |
        bash -c "
          apk add --no-cache jq bash
          chmod +x /usr/local/bin/init-kafka-topics.sh
          /usr/local/bin/init-kafka-topics.sh
        "

# Dependencies
dependencies:
  - zookeeper

# Ports that need to be available
required_ports:
  - "${KAFKA_PORT:-9092}"
  - "${ZOOKEEPER_PORT:-2181}"
  - "${KAFKA_UI_PORT:-8080}"

# Volume mounts
volumes:
  - name: kafka-data
    mount: /var/lib/kafka/data
    description: Kafka topic data and logs
  - name: zookeeper-data
    mount: /var/lib/zookeeper/data
    description: Zookeeper data directory
  - name: zookeeper-logs
    mount: /var/lib/zookeeper/log
    description: Zookeeper transaction logs

# Web interfaces
web_interfaces:
  - name: Kafka UI
    url: "http://localhost:${KAFKA_UI_PORT:-8080}"
    description: Kafka cluster management and topic browser

# Common CLI commands
cli_commands:
  connect: "docker exec -it ${PROJECT_NAME:-dev-stack}-kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning"
  create_topic: "docker exec ${PROJECT_NAME:-dev-stack}-kafka kafka-topics --create --bootstrap-server localhost:9092 --topic test --partitions 3 --replication-factor 1"
  list_topics: "docker exec ${PROJECT_NAME:-dev-stack}-kafka kafka-topics --list --bootstrap-server localhost:9092"
  describe_topic: "docker exec ${PROJECT_NAME:-dev-stack}-kafka kafka-topics --describe --bootstrap-server localhost:9092 --topic"
  produce_message: "docker exec -it ${PROJECT_NAME:-dev-stack}-kafka kafka-console-producer --bootstrap-server localhost:9092 --topic test"

# Documentation links
docs:
  - name: Apache Kafka Documentation
    url: https://kafka.apache.org/documentation/
  - name: Spring for Apache Kafka
    url: https://docs.spring.io/spring-kafka/docs/current/reference/html/
  - name: Kafka Quickstart
    url: https://kafka.apache.org/quickstart
  - name: Confluent Platform
    url: https://docs.confluent.io/platform/current/overview.html

# Common use cases
use_cases:
  - Event-driven microservices architecture
  - Real-time data streaming and processing
  - Application event sourcing
  - Log aggregation and processing
  - Message queuing between services
  - Data pipeline and ETL processes
  - Activity tracking and analytics
  - Pub/sub messaging patterns

# Integration notes
integration_notes:
  - "Includes Zookeeper for cluster coordination (required for Kafka)"
  - "Auto-creates topics when messages are published (configurable)"
  - "Default configuration suitable for development workloads"
  - "Kafka UI included for easy topic and message management"
  - "Single-node setup with replication factor 1"
  - "For production, use external Zookeeper and multiple Kafka brokers"

# Required dependencies
required_dependencies:
  maven:
    - "org.springframework.kafka:spring-kafka"
    - "org.apache.kafka:kafka-clients"
  gradle:
    - "implementation 'org.springframework.kafka:spring-kafka'"
    - "implementation 'org.apache.kafka:kafka-clients'"

# Performance and resource notes
performance_notes:
  - "Kafka requires significant memory (1GB+ recommended)"
  - "Startup time can be 45-60 seconds due to Zookeeper initialization"
  - "Topics are auto-created with 3 partitions by default"
  - "Log retention set to 7 days for development use"
  - "Consider adjusting memory limits based on expected throughput"

# Custom topics configuration
topics_configuration:
  description: "Custom topics to create automatically"
  example:
    - name: "user-events"
      partitions: 3
      replication_factor: 1
      cleanup_policy: "delete"
      retention_ms: 604800000 # 7 days
    - name: "order-processing"
      partitions: 6
      replication_factor: 1
    - name: "notifications"
      # Uses defaults: partitions: 1, replication_factor: 1

# Default topics created (when no custom topics specified)
default_topics:
  - name: "test"
    partitions: 3
    replication_factor: 1
    description: "Default test topic for development"
  - name: "events"
    partitions: 6
    replication_factor: 1
    description: "General application events topic"
  - name: "user-events"
    partitions: 3
    replication_factor: 1
    description: "User-related events topic"
  - name: "notifications"
    partitions: 3
    replication_factor: 1
    description: "Notifications topic"

# Configuration schema for custom topics
configuration_schema:
  topics:
    type: "array"
    description: "List of Kafka topics to create"
    items:
      name:
        type: "string"
        required: true
        description: "Topic name"
      partitions:
        type: "integer"
        default: 1
        description: "Number of partitions"
      replication_factor:
        type: "integer"
        default: 1
        description: "Replication factor (should not exceed broker count)"
      cleanup_policy:
        type: "string"
        default: "delete"
        enum: ["delete", "compact", "compact,delete"]
        description: "Topic cleanup policy"
      retention_ms:
        type: "integer"
        optional: true
        description: "Message retention time in milliseconds"
      segment_ms:
        type: "integer"
        optional: true
        description: "Segment file time threshold in milliseconds"

# Validation warnings
validation:
  warnings:
    - condition: "high_memory_usage"
      message: "Kafka requires significant memory (1GB+). Ensure adequate system resources."
    - condition: "slow_startup"
      message: "Kafka can take 45-60 seconds to start due to Zookeeper initialization."
    - condition: "high_partition_count"
      message: "Topics with many partitions may impact broker performance. Consider broker resources."
      threshold: 10
    - condition: "replication_factor_exceeds_brokers"
      message: "Replication factor cannot exceed number of brokers. Using single-broker setup (replication factor should be 1)."
